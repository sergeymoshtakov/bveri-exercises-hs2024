{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "header",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHQiU-VpRQeT",
        "outputId": "b038da44-602a-47ee-8439-f46a214dd151"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\n",
        "\n",
        "try:\n",
        "    import jupyter_black\n",
        "\n",
        "    jupyter_black.load()\n",
        "except:\n",
        "    print(\"black not installed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "title",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": [],
        "id": "hLNiqRqsRQeT"
      },
      "source": [
        "# PyTorch and Images\n",
        "\n",
        "## Goals\n",
        "\n",
        "- Know / Refresh the different components of a PyTorch ML Pipeline: Dataset, Data-Transforms, Data-Loader, Loss-Function, Logging & Metrics, Trainer, Evaluation\n",
        "- Be able to read, visualize and inspect images.\n",
        "- Train a neural network (MLP) on images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-a9856bbf98ea570f",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "pfNPHRDCRQeU"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Let's define paths, install & load the necessary Python packages.\n",
        "\n",
        "**Optionally: Save the notebook to your personal google drive to persist changes.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyQiZs90RQeU"
      },
      "source": [
        "Mount your google drive to store data and results (if running the code in Google Colab)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RjVWfbpARQeU",
        "outputId": "a41d20ce-6012-482e-f7cc-ba67eca86940"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In colab: True\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    import google.colab\n",
        "\n",
        "    IN_COLAB = True\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "\n",
        "print(f\"In colab: {IN_COLAB}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDgQhBqkRQeU",
        "outputId": "f035dd61-6b22-4f83-eb17-36af9cf9dfa7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "\n",
        "    drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fky2q1lqRQeU"
      },
      "source": [
        "**Modify the following paths if necessary.**\n",
        "\n",
        "That is where your data will be stored."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "aTtH4RjFRQeU"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "if IN_COLAB:\n",
        "    DATA_PATH = Path(\"/content/drive/MyDrive/bveri\")\n",
        "else:\n",
        "    DATA_PATH = Path(\"/workspace/code/data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRuJPdjlRQeU"
      },
      "source": [
        "Install the package `dl_cv_lectures`.\n",
        "\n",
        "The following code installs the package from a local repository (if available), otherwise it installs it from the exercise repository."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aLwnLFEeRQeV",
        "outputId": "06b768b6-f5e7-42d2-e6c0-49cec49fdb47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing from git repo\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "try:\n",
        "    import dl_cv_lectures\n",
        "\n",
        "    print(\"dl_cv_lectures installed, all good\")\n",
        "except ImportError as e:\n",
        "    import os\n",
        "\n",
        "    if Path(\"/workspace/code/src\").exists():\n",
        "        print(\"Installing from local repo\")\n",
        "        os.system(\"cd /workspace/code  && pip install .\")\n",
        "    else:\n",
        "        print(\"Installing from git repo\")\n",
        "        os.system(\"pip install git+https://github.com/i4Ds/bveri-exercises-hs2024\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jXmsO2RRQeV"
      },
      "source": [
        "### Load all packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PEwBUhh_RQeV"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import math\n",
        "import random\n",
        "from typing import Callable\n",
        "\n",
        "import requests\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchinfo\n",
        "import torchshow as ts\n",
        "import torchvision\n",
        "from matplotlib import pyplot as plt\n",
        "from PIL import Image\n",
        "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms.v2 import functional as TF\n",
        "from torchvision.utils import make_grid\n",
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTAIURdkRQeV"
      },
      "source": [
        "Define a default device for your computations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RoDTnTfERQeV"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rh-zfXWRQeV"
      },
      "source": [
        "## 1) Datasets: Define and Visualize\n",
        "\n",
        "A dataset is a collection of observations (including labels) to train, evaluate or test a model.\n",
        "\n",
        "Obtaining, organizing and defining datasets is an important step in data modeling. In the following, you will use PyTorch classes to create such datasets.\n",
        "\n",
        "You can find more information in this tutorial:  [https://pytorch.org/tutorials/beginner/basics/data_tutorial.html](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html).\n",
        "\n",
        "In particular, you should be familiar with, understand, and be able to use the classes [torch.utils.data.Dataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) and [torch.utils.data.DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhGl1cU0RQeV"
      },
      "source": [
        "### Create a dataset from torchvision\n",
        "\n",
        "We can easily pull / create a dataset from the [torchvision](https://pytorch.org/vision/stable/index.html) library.\n",
        "\n",
        "This package contains pre-packaged datasets which can be used for academic or testing purposes.\n",
        "\n",
        "We download and create the `torchvision.datasets.MNIST` dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5baJ-i6bRQeV"
      },
      "outputs": [],
      "source": [
        "ds_mnist_train = torchvision.datasets.MNIST(\n",
        "    root=DATA_PATH,\n",
        "    train=True,\n",
        "    download=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUs9iNRVRQeV"
      },
      "source": [
        "Often, the ready-made datasets come already with train and test splits.\n",
        "\n",
        "We also obtain the `test` split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AqtqtWDfRQeV"
      },
      "outputs": [],
      "source": [
        "ds_mnist_test = torchvision.datasets.MNIST(root=DATA_PATH, train=False, download=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2JppSLYRQeV"
      },
      "source": [
        "### Look at individual observations\n",
        "\n",
        "[torch.utils.data.Dataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) is either an iterable object or a map-style object (more common). The differences are described here [Link](https://pytorch.org/docs/stable/data.html#dataset-types).\n",
        "\n",
        "Our MNIST dataset is a map-style dataset (`_ _ getitem _ _` method implemented). This means you can retrieve any item using its key or index. We can also inspect its size.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cl6bD4uZRQeV"
      },
      "source": [
        "To access the first element of the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Khfx0p6URQeV"
      },
      "outputs": [],
      "source": [
        "ds_mnist_train[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7icl4KR-RQeV"
      },
      "source": [
        "**Question**: What makes up an observation in this case?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhweacmnRQeV"
      },
      "source": [
        "**Question**: How many elements are in the dataset? Check with `len()`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8GPaUJRRQeV"
      },
      "source": [
        "Now we visualize the first observation. The dataset returns a  [PIL.Image](https://pillow.readthedocs.io/en/stable/reference/Image.html) object, which is the most commonly used representation for images in PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fyb-KavQRQeV"
      },
      "outputs": [],
      "source": [
        "pil_image, label = ds_mnist_train[0]\n",
        "pil_image\n",
        "print(f\"Label: {label}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Geo-weyRQeV"
      },
      "source": [
        "We can also use [matplotlib](https://matplotlib.org/3.5.3/api/_as_gen/matplotlib.pyplot.html) to visualize images. The most commonly used plotting library in Python:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q7KX60xMRQeV"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(6, 6))\n",
        "_ = ax.imshow(pil_image, cmap=\"Greys_r\")\n",
        "_ = ax.axis(\"off\")\n",
        "_ = ax.set_title(f\"LABEL: {label}\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5MiUoqQRQeV"
      },
      "source": [
        "Or we can use [torchshow](https://github.com/xwying/torchshow). Be aware to convert the image to a `torch.Tensor` first. And scale the values between 0 and 1.\n",
        "`torchshow` provides a convenient way to visualize tensors. It also allows for visualizing batches of images.\n",
        "\n",
        "We use [torchvision.transforms.v2.functional](https://pytorch.org/vision/main/transforms.html#v2-api-reference-recommended) to convert a PIL image to a torch.Tensor.\n",
        "\n",
        "This module contains many useful image processing functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nJIdLXnKRQeV"
      },
      "outputs": [],
      "source": [
        "x = TF.to_image(pil_image).to(torch.float32) / 255.0\n",
        "ts.show(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XE6FQR87RQeW"
      },
      "source": [
        "Now we want to look at multiple images, to get a feeling for the dataset.\n",
        "\n",
        "Let's collect a couple of images. For convenience save them as tensors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C7cNqLUxRQeW"
      },
      "outputs": [],
      "source": [
        "def get_images_from_ds(\n",
        "    ds: torch.utils.data.Dataset, num_images_to_fetch: int = 16\n",
        ") -> list[torch.Tensor]:\n",
        "    \"\"\"Fetch first n images from a torch.utils.data.Dataset with (image, label) signature.\"\"\"\n",
        "    # for each image: convert it to (N x C x H x W) format and scale to 0-1\n",
        "    images = [\n",
        "        TF.to_image(ds[i][0]).to(torch.float32).unsqueeze(0) / 255.0\n",
        "        for i in range(0, num_images_to_fetch)\n",
        "    ]\n",
        "    return images\n",
        "\n",
        "\n",
        "images = get_images_from_ds(ds_mnist_train, num_images_to_fetch=16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tqfG-MWRQeW"
      },
      "source": [
        "A convenient way to display a grid of images is either `torchshow` or `torchvision.utils.make_grid`.\n",
        "\n",
        "We create a batch of images by concatenating them into a tensor of (N x C x H x W).\n",
        "\n",
        "**Note**: The most common way to organize images in PyTorch is the **NCHW** format, indicating the first dimension is the batch dimension, followed by the color channels, and height and width."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cevALVszRQeW"
      },
      "outputs": [],
      "source": [
        "x_batch = torch.concat(images)\n",
        "x_batch.shape\n",
        "ts.show(x_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpddUt74RQeW"
      },
      "source": [
        "Similarly we can create one big tensor with the images arranged in a grid and then visualize that using [torchvision.utils.make_grid](https://pytorch.org/vision/main/generated/torchvision.utils.make_grid.html).\n",
        "\n",
        "This gives us more control on how we want to arrange the images (`nrow` is the number of images per row)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U8l93kFQRQeW"
      },
      "outputs": [],
      "source": [
        "image_grid = make_grid(x_batch, nrow=2)\n",
        "ts.show(image_grid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UR-du61gRQeW"
      },
      "source": [
        "**Extra Task**: Visualize the images with their label on top of it to inspect label noise / correctness.\n",
        "\n",
        "You can use the following function to do the visualization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bEXsct3XRQeW"
      },
      "outputs": [],
      "source": [
        "def plot_square_collage_with_captions(\n",
        "    images: list[torch.Tensor], captions: list[str], caption_width: int = 30\n",
        "):\n",
        "    \"\"\"Plot Square collage of images with captions on to of each image.\"\"\"\n",
        "    import math\n",
        "    from textwrap import wrap\n",
        "\n",
        "    num_images = len(images)\n",
        "    side_length = math.ceil(math.sqrt(num_images))\n",
        "\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    for i in range(num_images):\n",
        "        ax = plt.subplot(side_length, side_length, i + 1)\n",
        "        caption = captions[i]\n",
        "        caption = \"\\n\".join(wrap(caption, caption_width))\n",
        "        plt.title(caption)\n",
        "        pil_image = TF.to_pil_image(images[i])\n",
        "        plt.imshow(pil_image)\n",
        "        plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QkXXZx2RQeW"
      },
      "source": [
        "**Extra Task**: Produce and inspect the distribution over the labels. This is important information and informs the whole modelling process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFbd4JPtRQeW"
      },
      "source": [
        "## 2) Data Transforms\n",
        "\n",
        "We often need to transform / pre-process our data.\n",
        "\n",
        "For example, we might want to scale the data such that the mean is 0 and the standard deviation is 1, or we want to bring the data into the $[0,1]$ interval (by dividing by 255.).\n",
        "\n",
        "Often we also want to apply `data augmentation` techniques to add more variation to our data.\n",
        "\n",
        "**We will look into this topic more at a later stage.** Below a small examle.\n",
        "\n",
        "Torchvision provides us with many options to implement transformations: [torchvision.transforms](https://pytorch.org/vision/main/transforms.html#)\n",
        "\n",
        "Here is a small example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vRj7SmhPRQeW"
      },
      "outputs": [],
      "source": [
        "url = \"https://github.com/pytorch/vision/blob/main/gallery/assets/dog2.jpg?raw=true\"\n",
        "r = requests.get(url, allow_redirects=True)\n",
        "image = Image.open(io.BytesIO(r.content))\n",
        "image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9QjE7kRRQeW"
      },
      "source": [
        "Now we define a transformation pipeline with random components and see what happens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_AmYikgpRQeW"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "composed_transforms = transforms.Compose(\n",
        "    [\n",
        "        transforms.RandomRotation(45),\n",
        "        transforms.RandomHorizontalFlip(0.5),\n",
        "        transforms.Resize((512, 512), antialias=True),\n",
        "        transforms.ToTensor(),\n",
        "    ]\n",
        ")\n",
        "\n",
        "num_examples = 16\n",
        "images = list()\n",
        "for i in range(0, num_examples):\n",
        "    images.append(composed_transforms(image))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7ABP8H9RQeX"
      },
      "source": [
        "Now we visualize the effect of the transforms on the image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-c783cb332598b935",
          "locked": true,
          "points": 0,
          "schema_version": 3,
          "solution": false,
          "task": true
        },
        "id": "E_f5nBnqRQeX"
      },
      "outputs": [],
      "source": [
        "images_batch = torch.stack(images, axis=0).to(torch.float32)\n",
        "images_batch.shape\n",
        "ts.show(images_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gDqbjhcRQeX"
      },
      "source": [
        "**Task**: Play around with different versions of the `composed_transforms` object. You find inspiration here: [torchvision.transforms](https://pytorch.org/vision/0.9/transforms.html#).\n",
        "\n",
        "A common operation is, e.g. a center crop."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsGk9bgVRQeX"
      },
      "source": [
        "## Data Loaders\n",
        "\n",
        "To train models we need to define a pipeline that reads, transforms and batches images.\n",
        "\n",
        "To achieve this we use the [torch.utils.data.DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) class.\n",
        "\n",
        "We can wrap a `torch.utils.data.Dataset` within a `torch.utils.data.DataLoader`.\n",
        "\n",
        "Note the options: `batch_size`, `shuffle` and `num_workers` which are important."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Erl2a460RQeX"
      },
      "outputs": [],
      "source": [
        "dl_mnist_train = torch.utils.data.DataLoader(\n",
        "    ds_mnist_train, batch_size=12, shuffle=True, num_workers=4\n",
        ")\n",
        "\n",
        "dl_mnist_test = torch.utils.data.DataLoader(\n",
        "    ds_mnist_test, batch_size=12, shuffle=False, num_workers=4\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWsjeA11RQeX"
      },
      "outputs": [],
      "source": [
        "# Let's check the first batch\n",
        "try:\n",
        "    images, labels = next(iter(dl_mnist_train))\n",
        "except TypeError as e:\n",
        "    print(f\"Error occurred: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "du3Ik3jxRQeX"
      },
      "source": [
        "**Question**: This does not work. Why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTaz5amwRQeX"
      },
      "source": [
        "**Answer**: The `Dataset` object returns `PIL.Image.Image` objects which can not be batched. We need to transform the images to tensors.\n",
        "\n",
        "\n",
        "We can use the `torchvision.transforms` module for that.\n",
        "\n",
        "\n",
        "`torchvision.datasets` offer a convenient way to apply transformations to images in a `Dataset` object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xVAOOmmWRQeX"
      },
      "outputs": [],
      "source": [
        "ds_mnist_train = torchvision.datasets.MNIST(\n",
        "    root=DATA_PATH, train=True, download=True, transform=transforms.ToTensor()\n",
        ")\n",
        "dl_mnist_train = torch.utils.data.DataLoader(\n",
        "    ds_mnist_train, batch_size=12, shuffle=True, num_workers=4\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGoxu2gIRQeX"
      },
      "source": [
        "Lets check if it worked."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4pugd1BIRQeX"
      },
      "outputs": [],
      "source": [
        "# Let's check the first batch\n",
        "try:\n",
        "    images, labels = next(iter(dl_mnist_train))\n",
        "except TypeError as e:\n",
        "    print(f\"Error occurred: {e}\")\n",
        "ts.show(images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yrf7LxQ3RQeX"
      },
      "source": [
        "Great! We are have now a data pipeline which can be used in model training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kzbc4inQRQeX"
      },
      "source": [
        "## 3) Network Definition\n",
        "\n",
        "Now, that we have prepared our data: We want to model it.\n",
        "\n",
        "First, we need to define our model: Let's impement a shallow Multilayer-Perceptron.\n",
        "\n",
        "We use the [torch.nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) class to define networks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x34YBi31RQeX"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    \"\"\"A Multi-Layer Perceptron (MLP) model for classification.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_hidden: int,\n",
        "        num_classes: int,\n",
        "        input_size: tuple[int, int, int] = (1, 28, 28),\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            num_hidden (int): Number of neurons in the hidden layer.\n",
        "            num_classes (int): Number of output classes for classification.\n",
        "            input_size tuple[int, int, int]: The dimensions of the input image.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # Flatten the input image into a 1D tensor\n",
        "        # for example, from (1, 28, 28) to (784,)\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        # Hidden layer: fully connected layer from input_size to num_hidden neurons.\n",
        "        # No bias is used here (bias=False).\n",
        "        self.hidden = nn.Linear(\n",
        "            in_features=input_size[0] * input_size[1] * input_size[2],\n",
        "            out_features=num_hidden,\n",
        "            bias=False,\n",
        "        )\n",
        "\n",
        "        # Output layer: fully connected layer from num_hidden neurons to num_classes outputs.\n",
        "        self.output = nn.Linear(in_features=num_hidden, out_features=num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through the MLP model.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor (batch_size, channels, height, width).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output logits (before softmax).\n",
        "        \"\"\"\n",
        "        # Flatten the input tensor into (batch_size, input_size[0] * input_size[1])\n",
        "        x = self.flatten(x)\n",
        "\n",
        "        # Apply the hidden layer (linear transformation)\n",
        "        x = self.hidden(x)\n",
        "\n",
        "        # Apply ReLU activation function to introduce non-linearity\n",
        "        x = F.relu(x)\n",
        "\n",
        "        # Apply the output layer (linear transformation) to get the logits\n",
        "        x = self.output(x)\n",
        "\n",
        "        # Return the output logits (not yet passed through softmax)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Fb_3LxhRQeX"
      },
      "source": [
        "Lets initialize the model and inspect it using `torchinfo`.\n",
        "\n",
        "Note: We need to define the input shape."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WLhWtWO2RQeX"
      },
      "outputs": [],
      "source": [
        "net = MLP(num_hidden=64, num_classes=10)\n",
        "net = net.to(device)\n",
        "print(net)\n",
        "print(torchinfo.summary(net, input_size=(1, 1, 28, 28)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjwkMNsiRQeX"
      },
      "source": [
        "**Question**: How many parameters does the model have?\n",
        "\n",
        "**Question**: How many would it have if there were only 5 classes? (`num_classes` from 10 to 5)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NA94_CNvRQeX"
      },
      "source": [
        "Let's check if the model works. E.g. if we can perform a forward pass."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1RQ5JEO1RQeX"
      },
      "outputs": [],
      "source": [
        "random_input = torch.randn(size=(1, 1, 28, 28)).to(device)\n",
        "\n",
        "output = net(random_input)\n",
        "\n",
        "output.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uylair_ZRQeX"
      },
      "source": [
        "**Question**: What would be the output.shape if we change to `torch.randn(size=(5, 1, 28, 28))`?\n",
        "\n",
        "**Question**: What would be the output.shape if we change to `torch.randn(size=(1, 3, 28, 28))`?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eiC4ArkrRQeX"
      },
      "source": [
        "## 4) Model Training\n",
        "\n",
        "We need different components to train a model:\n",
        "- Loss function and optimizer\n",
        "- Trainer: Iteration over training data and parameter updates\n",
        "- Monitoring of the training process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLEI0M_ZRQeY"
      },
      "source": [
        "### Loss Function & Optimizer\n",
        "\n",
        "We need to define a loss function and an optimizer for the model which defines how the parameters are updated.\n",
        "\n",
        "We use the Cross-Entropy Loss which is common for classification problems (more on that later)\n",
        "\n",
        "To optimize the neural network weights we use the popular Adam optimizer with regularization (weight decay)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWzQK8BRRQeY"
      },
      "outputs": [],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(net.parameters(), weight_decay=1e-3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-8F9THfRQeY"
      },
      "source": [
        "### Training Loop\n",
        "\n",
        "There are different ways to implement the training loop.\n",
        "\n",
        "It is typically a lot of boiler-plate code, thats why there are higher-level APIs such as:\n",
        "\n",
        "- [lightning](https://lightning.ai/docs/pytorch/stable/)\n",
        "- [Keras](https://keras.io/keras_3/)\n",
        "\n",
        "And also use metrics tracker from libraries such as [torchmetrics](https://github.com/Lightning-AI/torchmetrics).\n",
        "\n",
        "It is important to monitor training-progress. Thats why we calculate and print metrics.\n",
        "\n",
        "We are, however, implementing the boiler plate below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5x2IvxmvRQeY"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "\n",
        "def train_one_epoch(\n",
        "    data_loader: torch.utils.data.DataLoader,\n",
        "    net: torch.nn.Module,\n",
        "    optimizer: torch.optim.Adam,\n",
        "    loss_fn: Callable,\n",
        "    device: str = \"cpu\",\n",
        "    verbose: bool = True,\n",
        "):\n",
        "\n",
        "    net = net.to(device)\n",
        "\n",
        "    with tqdm(data_loader, unit=\"batch\", disable=not verbose) as tepoch:\n",
        "\n",
        "        total_samples_seen = 0\n",
        "        total_correct = 0\n",
        "\n",
        "        for step, (X, y) in enumerate(tepoch):\n",
        "\n",
        "            # Update Step\n",
        "            logits = net(X.to(device))\n",
        "            loss = loss_fn(logits, y.to(device))\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Calculate Accuracy\n",
        "            class_probabilities = torch.softmax(logits, axis=-1).detach().cpu()\n",
        "            y_hat = class_probabilities.argmax(dim=1, keepdim=True).squeeze().detach().cpu()\n",
        "\n",
        "            num_correct = (y_hat == y).sum().item()\n",
        "            num_samples = X.shape[0]\n",
        "            batch_accuracy = num_correct / num_samples\n",
        "\n",
        "            # Epoch Statistics\n",
        "            total_samples_seen += num_samples\n",
        "            total_correct += num_correct\n",
        "            epoch_accuracy = total_correct / total_samples_seen\n",
        "\n",
        "            if verbose:\n",
        "                tepoch.set_postfix(\n",
        "                    loss=loss.item(),\n",
        "                    accuracy_batch=batch_accuracy,\n",
        "                    accuracy_epoch=epoch_accuracy,\n",
        "                )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gdZVS5ARQeY"
      },
      "source": [
        "In more complex setups one often creates a `Trainer` class which manages the training process. See [lightning.Trainer](https://lightning.ai/docs/pytorch/stable/common/trainer.html) for an example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwuqeW14RQeY"
      },
      "source": [
        "### Optimization\n",
        "\n",
        "Now we train our model for a few epochs.\n",
        "\n",
        "We increase the batch_size to speed up the training.\n",
        "\n",
        "We also normalize the input images to speed up convergence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x7n3pTd0RQeY"
      },
      "outputs": [],
      "source": [
        "image_transforms = transforms.Compose(\n",
        "    [\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=0.5, std=0.5),\n",
        "    ]\n",
        ")\n",
        "\n",
        "ds_mnist_train = torchvision.datasets.MNIST(\n",
        "    root=DATA_PATH, train=True, download=True, transform=image_transforms\n",
        ")\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "dl_mnist_train = torch.utils.data.DataLoader(\n",
        "    ds_mnist_train, batch_size=128, shuffle=True, num_workers=4\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apwsXMsWRQeY"
      },
      "outputs": [],
      "source": [
        "total_epochs = 5\n",
        "for epoch in range(0, total_epochs):\n",
        "    print(f\"Starting Epoch: {epoch + 1} / {total_epochs}\")\n",
        "    train_one_epoch(dl_mnist_train, net, optimizer, loss_fn, device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6FLB0g6RQeY"
      },
      "source": [
        "Note the progress the model is making with respect to accuracy and the loss function value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NicJT0BURQeY"
      },
      "source": [
        "## 5) Evaluation\n",
        "\n",
        "A very important part is model evaluation. We can not rely on the training scores for model evaluation since it might be too optimistic due to overfitting.\n",
        "\n",
        "We evaluate our (best) model on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XsyoLizXRQeY"
      },
      "outputs": [],
      "source": [
        "def eval_loop(\n",
        "    data_loader: torch.utils.data.DataLoader,\n",
        "    net: torch.nn.Module,\n",
        "    loss_fn: Callable,\n",
        "    device: str = \"cpu\",\n",
        ") -> tuple[float, torch.Tensor, torch.Tensor]:\n",
        "\n",
        "    net = net.to(device)\n",
        "    net.eval()\n",
        "    with tqdm(data_loader, unit=\"batch\") as tepoch:\n",
        "\n",
        "        total_samples_seen = 0\n",
        "        total_correct = 0\n",
        "\n",
        "        y_list = list()\n",
        "        y_hat_list = list()\n",
        "\n",
        "        for step, (X, y) in enumerate(tepoch):\n",
        "\n",
        "            # Forward Pass\n",
        "            with torch.no_grad():\n",
        "                logits = net(X.to(device))\n",
        "            loss = loss_fn(logits, y.to(device))\n",
        "\n",
        "            # Predictions\n",
        "            class_probabilities = torch.softmax(logits, axis=-1).detach().cpu()\n",
        "            y_hat = class_probabilities.argmax(dim=1, keepdim=True).squeeze().detach().cpu()\n",
        "\n",
        "            # Metrics\n",
        "            num_correct = (y_hat == y).sum().item()\n",
        "            num_samples = X.shape[0]\n",
        "            total_samples_seen += num_samples\n",
        "            total_correct += num_correct\n",
        "            epoch_accuracy = total_correct / total_samples_seen\n",
        "\n",
        "            tepoch.set_postfix(\n",
        "                loss=loss.item(),\n",
        "                accuracy_epoch=epoch_accuracy,\n",
        "            )\n",
        "\n",
        "            # save preds and targets\n",
        "            y_list.append(y.cpu())\n",
        "            y_hat_list.append(y_hat.cpu())\n",
        "\n",
        "    return epoch_accuracy, torch.concat(y_list), torch.concat(y_hat_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkxKuJQVRQeY"
      },
      "source": [
        "When testing and evaluating we need to apply exactly the same (static) transformations (not the data augmentation!) as when training the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dyo1MBrgRQeY"
      },
      "outputs": [],
      "source": [
        "ds_mnist_test = torchvision.datasets.MNIST(\n",
        "    root=DATA_PATH, train=False, download=True, transform=image_transforms\n",
        ")\n",
        "dl_mnist_test = torch.utils.data.DataLoader(\n",
        "    ds_mnist_test, batch_size=128, shuffle=False, num_workers=4\n",
        ")\n",
        "\n",
        "test_accuracy, y, y_hat = eval_loop(dl_mnist_test, net, loss_fn, device=device)\n",
        "\n",
        "print(f\"Test Accuracy:  {test_accuracy:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IblyLxEpRQeY"
      },
      "source": [
        "We also inspect the confusion matrix using the sklearn confusion matrix [Link](https://scikit-learn.org/1.5/modules/generated/sklearn.metrics.confusion_matrix.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2NAGPngORQeY"
      },
      "outputs": [],
      "source": [
        "cm = confusion_matrix(y, y_hat)\n",
        "\n",
        "cm_display = ConfusionMatrixDisplay(cm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ai0jfGQNRQeY"
      },
      "outputs": [],
      "source": [
        "cm_display.plot()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}