{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "header",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "try:\n",
    "    import jupyter_black\n",
    "\n",
    "    jupyter_black.load()\n",
    "except:\n",
    "    print(\"black not installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "title",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "# Multilayer-Perceptron and Images\n",
    "\n",
    "## Goals\n",
    "\n",
    "- Train an MLP and understand how it works on images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a9856bbf98ea570f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Setup\n",
    "\n",
    "Let's define paths, install & load the necessary Python packages.\n",
    "\n",
    "**Optionally: Save the notebook to your personal google drive to persist changes.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mount your google drive to store data and results (if running the code in Google Colab)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "\n",
    "print(f\"In colab: {IN_COLAB}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "\n",
    "    drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modify the following paths if necessary.**\n",
    "\n",
    "That is where your data will be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "if IN_COLAB:\n",
    "    DATA_PATH = Path(\"/content/drive/MyDrive/bveri\")\n",
    "else:\n",
    "    DATA_PATH = Path(\"/workspace/code/data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the package `dl_cv_lectures`.\n",
    "\n",
    "The following code installs the package from a local repository (if available), otherwise it installs it from the exercise repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import dl_cv_lectures\n",
    "\n",
    "    print(\"dl_cv_lectures installed, all good\")\n",
    "except ImportError as e:\n",
    "    import os\n",
    "\n",
    "    if Path(\"/workspace/code/src\").exists():\n",
    "        print(\"Installing from local repo\")\n",
    "        os.system(\"cd /workspace/code  && pip install .\")\n",
    "    else:\n",
    "        print(\"Installing from git repo\")\n",
    "        os.system(\"pip install git+https://github.com/i4Ds/bveri-exercises-hs2024\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load all packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "from typing import Callable\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchinfo\n",
    "import torchshow as ts\n",
    "import torchvision\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms.v2 import functional as TF\n",
    "from torchvision.utils import make_grid\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a default device for your computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Define Data, Loaders, and MLP\n",
    "\n",
    "Quickly get data and train an MLP.\n",
    "\n",
    "We use some pre-defined functionality from the `dl_cv_lectures` package. Feel free to substitute with your own functionality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dl_cv_lectures import visualize\n",
    "from dl_cv_lectures.classification import train_one_epoch\n",
    "from dl_cv_lectures.data import pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "We use a dataset that consists of inspection images of microscopic components. Some of them are faulty (label=1) and need to be identified and sorted out.\n",
    "\n",
    "Lets load the data and take a look at it.\n",
    "\n",
    "First we create a [torch.utils.data.Dataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = pattern.PatternDataset(\n",
    "    num_samples=100, image_side_length=16, seed=123, max_errors=3, max_x_y_shift=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the first observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = ds_train[0]\n",
    "image.size\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We collect a few samples, visualize them and try to understand how label and images are related."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images_and_labels_from_ds(\n",
    "    ds: torch.utils.data.Dataset, num_images_to_fetch: int = 16\n",
    ") -> list[torch.Tensor]:\n",
    "    \"\"\"Fetch first n images from a torch.utils.data.Dataset with (image, label) signature.\"\"\"\n",
    "    # for each image: convert it to (C x H x W) format and scale to 0-1\n",
    "    images = [\n",
    "        TF.to_image(ds[i][0]).to(torch.float32) / 255.0 for i in range(0, num_images_to_fetch)\n",
    "    ]\n",
    "    labels = [ds[i][1] for i in range(0, num_images_to_fetch)]\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = get_images_and_labels_from_ds(ds_train, num_images_to_fetch=16)\n",
    "\n",
    "fig, ax = visualize.plot_square_collage_with_captions(\n",
    "    images, [f\"Label: {label}\" for label in labels], global_normalize=True\n",
    ")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: When is a pattern label=0, when is it label=1?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP Architecture\n",
    "\n",
    "Next, we define an MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"A Multi-Layer Perceptron (MLP) model for classification.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_hidden: int,\n",
    "        num_classes: int,\n",
    "        input_size: tuple[int, int, int] = (1, 28, 28),\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_hidden (int): Number of neurons in the hidden layer.\n",
    "            num_classes (int): Number of output classes for classification.\n",
    "            input_size tuple[int, int, int]: The dimensions of the input image.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Flatten the input image into a 1D tensor\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Hidden layer: fully connected layer from input_size to num_hidden neurons.\n",
    "        self.hidden = nn.Linear(\n",
    "            in_features=input_size[0] * input_size[1] * input_size[2],\n",
    "            out_features=num_hidden,\n",
    "            bias=True,\n",
    "        )\n",
    "\n",
    "        # Output layer: fully connected layer from num_hidden neurons to num_classes outputs.\n",
    "        self.output = nn.Linear(in_features=num_hidden, out_features=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the MLP model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor (batch_size, channels, height, width).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output logits (before softmax).\n",
    "        \"\"\"\n",
    "        # Flatten the input tensor into (batch_size, input_size[0] * input_size[1])\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        # Apply the hidden layer (linear transformation)\n",
    "        x = self.hidden(x)\n",
    "\n",
    "        # Apply ReLU activation function to introduce non-linearity\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Apply the output layer (linear transformation) to get the logits\n",
    "        x = self.output(x)\n",
    "\n",
    "        # Return the output logits (not yet passed through softmax)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets initialize the model and inspect it using `torchinfo`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "net = MLP(num_hidden=16, num_classes=2, input_size=(1, 16, 16))\n",
    "print(net)\n",
    "print(torchinfo.summary(net, input_size=(1, 1, 16, 16)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define loss function and optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=1e-1, weight_decay=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the training dataset and dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "ds_train = pattern.PatternDataset(\n",
    "    num_samples=100000,\n",
    "    seed=123,\n",
    "    max_errors=3,\n",
    "    max_x_y_shift=0,\n",
    "    transform=image_transforms,\n",
    ")\n",
    "\n",
    "dl_train = torch.utils.data.DataLoader(ds_train, batch_size=128, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "That's it! We are ready to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_epochs = 3\n",
    "for epoch in range(0, total_epochs):\n",
    "    print(f\"Starting Epoch: {epoch + 1} / {total_epochs}\")\n",
    "    train_one_epoch(dl_train, net, optimizer, loss_fn, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Analyse MLP properties\n",
    "\n",
    "Now it is getting interesting! \n",
    "\n",
    "We want to inspect the weights of the MLP.\n",
    "\n",
    "We can access the layers of our model by the attributes that we defined.\n",
    "\n",
    "For example we can access the attribute `hidden` of our `MLP`  object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = net.hidden.weight\n",
    "weights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weight matrix $\\mathbf{W}$ is multiplied with an image $\\mathbf{x}$ to produce the layer activations $\\mathbf{a}$ with: $\\mathbf{a} = \\mathbf{x} \\mathbf{W}^T + \\mathbf{b}$\n",
    "\n",
    "We can see that the hidden layer has 16 neurons, each of which is connected to all input neurons.\n",
    "\n",
    "Each row in $\\mathbf{W}$ can be visualized as an image. We need to reshape it accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: What do you expect to see when visualizing the weights?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_weights(\n",
    "    weights: torch.Tensor, figsize: tuple[int, int] = (12, 12), scale_each: bool = True\n",
    "):\n",
    "    num_neurons = weights.shape[0]\n",
    "    dim = weights.shape[1]\n",
    "\n",
    "    side_length = int(math.sqrt(dim))\n",
    "\n",
    "    weights = weights.reshape(num_neurons, 1, side_length, side_length)\n",
    "\n",
    "    nrow = int(math.sqrt(num_neurons))\n",
    "    image_grid = TF.to_pil_image(\n",
    "        make_grid(weights, nrow=nrow, normalize=True, scale_each=scale_each)\n",
    "    )\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    _ = ax.imshow(image_grid, cmap=\"Greys_r\")\n",
    "    _ = ax.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "visualize_weights(weights, scale_each=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: How do you interpret the weights? How does it match with your expectations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: What happens if we reduce the hidden layer to just 2 neurons?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "net = MLP(num_hidden=2, num_classes=2, input_size=(1, 16, 16))\n",
    "dl_train = torch.utils.data.DataLoader(ds_train, batch_size=128, shuffle=True, num_workers=4)\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=1e-1, weight_decay=1e-3)\n",
    "total_epochs = 5\n",
    "for epoch in range(0, total_epochs):\n",
    "    print(f\"Starting Epoch: {epoch + 1} / {total_epochs}\")\n",
    "    train_one_epoch(dl_train, net, optimizer, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_weights(net.hidden.weight, scale_each=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) What if?\n",
    "\n",
    "What if we make it a bit more difficult and let the pattern randomly shift spatially?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = pattern.PatternDataset(num_samples=100000, seed=123, max_errors=3, max_x_y_shift=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = get_images_and_labels_from_ds(ds_train, num_images_to_fetch=16)\n",
    "\n",
    "fig, ax = visualize.plot_square_collage_with_captions(\n",
    "    images, [f\"Label: {label}\" for label in labels], global_normalize=True\n",
    ")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Is this a more difficult problem? How will the weights differ from the simpler case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "net = MLP(num_hidden=16, num_classes=2, input_size=(1, 16, 16))\n",
    "ds_train = pattern.PatternDataset(\n",
    "    num_samples=100000,\n",
    "    seed=123,\n",
    "    max_errors=3,\n",
    "    max_x_y_shift=1,\n",
    "    transform=image_transforms,\n",
    ")\n",
    "dl_train = torch.utils.data.DataLoader(ds_train, batch_size=128, shuffle=True, num_workers=4)\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=1e-1, weight_decay=1e-3)\n",
    "total_epochs = 5\n",
    "for epoch in range(0, total_epochs):\n",
    "    print(f\"Starting Epoch: {epoch + 1} / {total_epochs}\")\n",
    "    train_one_epoch(dl_train, net, optimizer, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_weights(net.hidden.weight, scale_each=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is already a bit more difficult to interpret!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Let's try somethig more crazy!\n",
    "\n",
    "We extend the images and place each pattern in a random quarant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dl_cv_lectures.transform import RandomQuadrantPad\n",
    "\n",
    "torch.manual_seed(123)\n",
    "ds_train = pattern.PatternDataset(\n",
    "    num_samples=100000,\n",
    "    seed=123,\n",
    "    max_errors=3,\n",
    "    max_x_y_shift=0,\n",
    "    transform=transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            RandomQuadrantPad(),\n",
    "            transforms.Lambda(lambda x: TF.to_pil_image(x)),\n",
    "        ]\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = get_images_and_labels_from_ds(ds_train, num_images_to_fetch=16)\n",
    "\n",
    "fig, ax = visualize.plot_square_collage_with_captions(\n",
    "    images, [f\"Label: {label}\" for label in labels], global_normalize=True\n",
    ")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: What do you think happens here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "net = MLP(num_hidden=16, num_classes=2, input_size=(1, 16 * 2, 16 * 2))\n",
    "ds_train = pattern.PatternDataset(\n",
    "    num_samples=100000,\n",
    "    seed=123,\n",
    "    max_errors=3,\n",
    "    max_x_y_shift=0,\n",
    "    transform=transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            RandomQuadrantPad(),\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "dl_train = torch.utils.data.DataLoader(ds_train, batch_size=128, shuffle=True, num_workers=4)\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=1e-1, weight_decay=1e-3)\n",
    "total_epochs = 5\n",
    "for epoch in range(0, total_epochs):\n",
    "    print(f\"Starting Epoch: {epoch + 1} / {total_epochs}\")\n",
    "    train_one_epoch(dl_train, net, optimizer, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_weights(net.hidden.weight, scale_each=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) What happens if we increase the positional uncertainty?\n",
    "\n",
    "Let's randomly shift the pattern by up to 3 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "ds_train = pattern.PatternDataset(\n",
    "    num_samples=100000,\n",
    "    seed=123,\n",
    "    max_errors=3,\n",
    "    max_x_y_shift=3,\n",
    "    transform=transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            RandomQuadrantPad(),\n",
    "            transforms.Lambda(lambda x: TF.to_pil_image(x)),\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "\n",
    "images, labels = get_images_and_labels_from_ds(ds_train, num_images_to_fetch=16)\n",
    "\n",
    "fig, ax = visualize.plot_square_collage_with_captions(\n",
    "    images, [f\"Label: {label}\" for label in labels], global_normalize=True\n",
    ")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We switch to the Adam optimizer which is often much faster and needs less care tuning learning rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "net = MLP(num_hidden=16, num_classes=2, input_size=(1, 16 * 2, 16 * 2))\n",
    "ds_train = pattern.PatternDataset(\n",
    "    num_samples=100000,\n",
    "    seed=123,\n",
    "    max_errors=3,\n",
    "    max_x_y_shift=3,\n",
    "    transform=transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            RandomQuadrantPad(),\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "dl_train = torch.utils.data.DataLoader(ds_train, batch_size=128, shuffle=True, num_workers=4)\n",
    "optimizer = torch.optim.Adam(net.parameters())\n",
    "total_epochs = 5\n",
    "for epoch in range(0, total_epochs):\n",
    "    print(f\"Starting Epoch: {epoch + 1} / {total_epochs}\")\n",
    "    train_one_epoch(dl_train, net, optimizer, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_weights(net.hidden.weight, scale_each=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) A Peek Ahead: What happens if we choose a CNN?\n",
    "\n",
    "Finally! Let's ditch the MLP.\n",
    "\n",
    "The following CNN is deeper but has 10 times fewer parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class SmallCNN(nn.Module):\n",
    "    def __init__(self, kernel_size: int, num_classes=10, num_filters=16):\n",
    "        super(SmallCNN, self).__init__()\n",
    "\n",
    "        padding = kernel_size // 2\n",
    "\n",
    "        # First convolutional layer: 1 input channel (grayscale), 16 output channels, 3x3 kernel\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=1,\n",
    "            out_channels=num_filters,\n",
    "            kernel_size=kernel_size,\n",
    "            padding=padding,\n",
    "        )\n",
    "\n",
    "        # Second convolutional layer: 16 input channels, 32 output channels, 3x3 kernel\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=num_filters,\n",
    "            out_channels=num_filters * 2,\n",
    "            kernel_size=kernel_size,\n",
    "            padding=padding,\n",
    "        )\n",
    "\n",
    "        # Third convolutional layer: 32 input channels, 64 output channels, 3x3 kernel\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            in_channels=num_filters * 2,\n",
    "            out_channels=num_filters * 4,\n",
    "            kernel_size=kernel_size,\n",
    "            padding=padding,\n",
    "        )\n",
    "\n",
    "        # Max pooling layer\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Global Average Pooling\n",
    "        self.gap = nn.AdaptiveAvgPool2d((1, 1))  # Output size is 1x1 per feature map\n",
    "\n",
    "        # Final output layer\n",
    "        self.fc = nn.Linear(in_features=num_filters * 4, out_features=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First conv layer with ReLU\n",
    "        x = F.relu(self.conv1(x))\n",
    "        # Second conv layer with ReLU and max pooling\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        # Third conv layer with ReLU and max pooling\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "\n",
    "        # Global Average Pooling (GAP)\n",
    "        x = self.gap(x)  # Shape will be (batch_size, 64, 1, 1)\n",
    "\n",
    "        # Flatten the GAP output to feed into the fully connected layer\n",
    "        x = torch.flatten(x, 1)  # Shape (batch_size, 64)\n",
    "\n",
    "        # Final fully connected layer (acts as the output layer)\n",
    "        x = self.fc(x)  # Shape (batch_size, num_classes)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = SmallCNN(kernel_size=3, num_classes=2, num_filters=2)\n",
    "print(torchinfo.summary(net, input_size=(1, 1, 16 * 2, 16 * 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "net = SmallCNN(kernel_size=3, num_classes=2, num_filters=2)\n",
    "ds_train = pattern.PatternDataset(\n",
    "    num_samples=100000,\n",
    "    seed=123,\n",
    "    max_errors=3,\n",
    "    max_x_y_shift=3,\n",
    "    transform=transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            RandomQuadrantPad(),\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "dl_train = torch.utils.data.DataLoader(ds_train, batch_size=128, shuffle=True, num_workers=6)\n",
    "optimizer = torch.optim.Adam(net.parameters())\n",
    "total_epochs = 3\n",
    "for epoch in range(0, total_epochs):\n",
    "    print(f\"Starting Epoch: {epoch + 1} / {total_epochs}\")\n",
    "    train_one_epoch(dl_train, net, optimizer, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CNN uses only two filters in the first layer, which we can visualize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts.show(net.conv1.weight)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
